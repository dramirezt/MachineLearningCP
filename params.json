{"name":"Machine Learning Course Project","tagline":"Machine Learning Course Project","body":"---\r\ntitle: \"Machine Learning - Course Project\"\r\nauthor: \"Daniel Ram√≠rez Torres\"\r\ndate: \"22/08/2015\"\r\noutput: html_document\r\n---\r\n\r\n## Introduction\r\n\r\nIn this project, we will use the information obtained from several accelerometers of 6 participants. The participants were asked to perform barebell lifts correctly an dincorrectly in 5 different ways. The goal of this project is to predict the manner in which they did the exercise. This is the \"classe\" variable in the training set.\r\n\r\nIn the end, the question to answer is: can we predict the quality of the activity?\r\n\r\n\r\n## Loading and exploring the data\r\n\r\nFirst we need to download our training and test sets, as well as load the caret package which will be used later.\r\n\r\nTraining set download: \r\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\r\n\r\nTest set download: \r\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\r\n\r\n\r\n```{r}\r\nlibrary(caret)\r\ndtrain <- read.csv(\"pml-training.csv\")\r\ndtest <- read.csv(\"pml-testing.csv\")\r\n```\r\n\r\nAs we want to build a predictor, we'll decompose our training set into two datasets (one for testing and the other to use as supervised training)\r\n\r\n```{r}\r\nset.seed(1234)\r\nsplit <- createDataPartition(y=dtrain$classe, p=0.8, list=FALSE)\r\ntraining <- dtrain[split, ]\r\ntesting <- dtrain[-split, ]\r\nncol(training)\r\n```\r\n\r\n\r\nAs we can see, we have a lot of features in our dataset. In the next section we will analyze them and clean the dataset in order to use only the relevant features.\r\n\r\nThe data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.\r\n\r\n\r\n## Cleaning the dataset\r\n\r\nAs we want to reduce the number of features (in both training and testing), we'll do it by removing variables with:\r\n\r\n- No intuitive sense in this prediciton.\r\n- Zero variance (or almost zero).\r\n- Most common value NA.\r\n\r\n```{r}\r\n# Removing the first 5 columns: X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp) as they dont make sense for prediction.\r\ntraining <- training[, -(1:5)]\r\ntesting <- testing[, -(1:5)]\r\n\r\n# Removing zero variance values\r\nindexZero <- nearZeroVar(testing)\r\ntraining <- training[, -indexZero]\r\ntesting <- testing[, -indexZero]\r\n\r\n# Removing variables with NA as most common value.\r\nindexNA <- sapply(training, function(x) mean(is.na(x))) > 0.9\r\ntraining <- training[, indexNA==FALSE]\r\ntesting <- testing[, indexNA==FALSE]\r\nncol(training)\r\n```\r\n\r\n## Modeling with Random Forests\r\n\r\nAs I have some experience with random forests, I have decided to create my model using them. I expect a high accuracy althoug not really good performance (as random forest is quite expensive). Let's build the model:\r\n\r\n```{r}\r\nlibrary(randomForest)\r\nmodel <- randomForest(classe ~. , data=training)\r\nmodel\r\n```\r\n\r\nNow we can predict our in-sample error using the predict function:\r\n\r\n```{r}\r\npredictions <- predict(model, testing, type=\"class\")\r\n```\r\n\r\nFinally, we can test our results using a confusion matrix:\r\n\r\n```{r}\r\nconfusionMatrix(predictions, testing$classe)\r\n```\r\n\r\nAs our accuracy is 99.8%, our predicted accuracy for the out-of-sample error is 0.2%.\r\n\r\n## Generating testfiles\r\n\r\n```{r}\r\noutput <- predict(model, dtest, type=\"class\")\r\npml_write_files = function(x){\r\n  n = length(x)\r\n  for(i in 1:n){\r\n    filename = paste0(\"problem_id_\",i,\".txt\")\r\n    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n  }\r\n}\r\npml_write_files(output)\r\n```","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}